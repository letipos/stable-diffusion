{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Stable Diffusion. What's the hype ? \n","\n","Diffusion models are quite in the hype for Text-to-Image generation. We all have seen the weird videos on the Internet of Text-to-Image generation, but how does it even work ?\n","\n","### Noising and Denoising : \n","We'll start with the easy part. Denoising refers to process of removing noise from the image and Noising refers to the process of adding noise to an image. That sounds easy. Can there be any problem in this simple task ?\n","\n","It turns out that generally, the image synthesis tasks are performed by deep geenrative models(such as GANs, VAEs, and autoregressive models) which has its downsides when trained to synthsize high quality samples on difficult, high resolution datasets. \n","\n","Maybe - we can just break down this process of noising and denoising into smaller, managable chunks ? ...  \n","\n","### What is Diffusion ?\n","\n","The answer to the last question is **Diffusion models**. \n","\n","The idea behind diffusion is quite simple. Firstly, we'll start with corrupting the training data iteratively by adding gaussian noise, slowly wiping out the details till it becomes pure noise, and then training a model to reverse this corruption process(which is also done iteratively) \n","\n","This process of de-corrupting the image is also quite clever and happens over several iterations. Instead of directly generating the image, (which can be quite not so accurate and have some ambiguities) we ask the denoising model(also called backward process) to try to predict the noise itself !(again, keep in mind that this is done at a particular iteration). Then , we just subtract the noise from the image.\n","\n","It turns out that this is much more accurate than directly generating the image. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## How do we go from Diffusion to Text-to-Image Generation ?\n","\n","This is also done in  a clever way. Together with image that needs to be diffused, we put together the text associated with the image (after converting it to embeddings) into a model and guide the diffusion model towards some target class. This gives us an denoised image which is closely related to the caption as well. However complicated it sounds, it is done through a rather simple and **VERY** clever technique called **Classifier Guidance**. \n","\n","### Classifier Guidance : \n","In Stable Diffusion, we make use of something called **CLIP** embeddings to guide the diffusion towards the target class during the training. CLIP stands for Contrastive Loss Image Pair. The ideas behind this is to make the image and word embeddings similar in their semantics. Similar CLIP embeddings are used in DALL-E as well.   \n","\n","\n","#### Generation of never seen images - Classifier Free Guidance :\n","One question that is often asked when people see Text-To-Image generation is - How can it come up with images it has never seen before ? As with everything in this notebook - this was also done using a clever technique called **Classifier Free Guidance**.\n","\n","In Classifier free guidance - instead of one noisy image, two same images are fed to model - one without the text embeddings and one with it. The diffusion model therefore comes up with two image - one without text embeddings and one without it. Together, these two noise images* can be used to amplifiy the signal and generate images which are previously not generated.\n","\n","\n","\n","\n","*Rememer that the model generates noise, not the actual image"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Phewwww !! That's a lot of theory !\n","Now we'll move on the implementation part. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-08T16:01:55.725254Z","iopub.status.busy":"2022-11-08T16:01:55.724553Z","iopub.status.idle":"2022-11-08T16:02:08.118091Z","shell.execute_reply":"2022-11-08T16:02:08.116783Z","shell.execute_reply.started":"2022-11-08T16:01:55.725221Z"},"trusted":true},"outputs":[],"source":["# Necessary Library for the notebook !\n","!pip install transformers diffusers lpips \n","\n","# For interactive notebooks !\n","!jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-08T16:02:21.749633Z","iopub.status.busy":"2022-11-08T16:02:21.749239Z","iopub.status.idle":"2022-11-08T16:02:21.941292Z","shell.execute_reply":"2022-11-08T16:02:21.940245Z","shell.execute_reply.started":"2022-11-08T16:02:21.749583Z"},"trusted":true},"outputs":[],"source":["# Stable Diffusion models are openly avaialble via HuggingFace\n","\n","# Login into your HuggingFace account through this notebook !\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-08T16:02:34.304170Z","iopub.status.busy":"2022-11-08T16:02:34.303682Z","iopub.status.idle":"2022-11-08T16:02:39.807548Z","shell.execute_reply":"2022-11-08T16:02:39.806495Z","shell.execute_reply.started":"2022-11-08T16:02:34.304133Z"},"trusted":true},"outputs":[],"source":["# Necessary Libraries : \n","\n","!huggingface-cli whoami  ## Just make sure you are logged in !\n","\n","import numpy\n","from tqdm.auto import tqdm\n","\n","import torch\n","from torch import autocast\n","from torchvision import transforms as tfms\n","\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import StableDiffusionPipeline\n","\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","\n","# For video display:\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","# For interactive notebook\n","import ipywidgets as widgets\n","from ipywidgets import interact, interact_manual\n","\n","# Set device - Make sure to set the runtime to GPU in the Kaggle Notebook !\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(torch_device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setting up the Diffusion Models and Text Embeddings.\n","\n","- VAE(Variational Auto Encoder) : The Encoder-Decoder Model for noising and denoising the image.\n","- Tokenizer : As discussed above, we will be using Contrastive Loss Image Pair(CLIP) embeddings.\n","- UNet : UNet model is for generating the latents\n","\n","This process is simplied in the `StableDiffusionPipeline` in the `diffusers` package. It implements all of VAE, Tokenizer, UNet in a single pipeline.\n","\n","Other things which need some setting up :\n","- For this, we can use LMS(Least Mean Square) Dicrete Scheduler."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-08T16:03:28.384182Z","iopub.status.busy":"2022-11-08T16:03:28.383464Z","iopub.status.idle":"2022-11-08T16:07:30.906650Z","shell.execute_reply":"2022-11-08T16:07:30.905631Z","shell.execute_reply.started":"2022-11-08T16:03:28.384140Z"},"trusted":true},"outputs":[],"source":["# Use the scheduler in stableDiff pipeline\n","# scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n","stable_diff_pipe = StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\n","\n","stable_diff_pipe.to(torch_device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Inference\n","\n","With the prompt now set, run the cell below!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-08T16:07:30.913797Z","iopub.status.busy":"2022-11-08T16:07:30.913485Z","iopub.status.idle":"2022-11-08T16:07:56.185024Z","shell.execute_reply":"2022-11-08T16:07:56.182704Z","shell.execute_reply.started":"2022-11-08T16:07:30.913771Z"},"trusted":true},"outputs":[],"source":["prompt = ['A monkey dancing in a club']   # Change this and Enjoy !!\n","guidance_scale = 12.5\n","\n","# Change the seed parameter to create an original image each time.\n","generator = torch.manual_seed(69)    # Nice !\n","\n","# Loop\n","with autocast(\"cuda\"):\n","    image = stable_diff_pipe(prompt, guidance_scale = guidance_scale).images[0]\n","    \n","\n","# Display the image.\n","image"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Resources :\n","\n","- [HuggingFace-Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion-v1-4)\n","- [Pytorch Documentation](https://pytorch.org/docs/stable/index.html)\n","- [Classifier-Free Guidance](https://openreview.net/pdf?id=qw8AKxfYbI)\n","- [How Diffusion Models Work ?](https://theaisummer.com/diffusion-models/)\n","- [High Fidelity Image Generation using Diffusion Models](https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html)\n","- [High Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf)\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
